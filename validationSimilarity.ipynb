{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Impostazioni di InferenceClient\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=\"your_HF_token\"\n",
    ")\n",
    "\n",
    "workflow_directory = \"/content/drive/My Drive/DatabaseTesi/TEST_RAG/test\"\n",
    "save_directory = \"/content/drive/My Drive/DatabaseTesi/TEST_RAG/afterValidationHalf\"\n",
    "directory = \"/content/drive/My Drive/DatabaseTesi/TEST_RAG/noAnnotation\"\n",
    "progress_file = \"/content/drive/My Drive/DatabaseTesi/TEST_RAG/progressHalf.json\" \n",
    "\n",
    "def read_files_from_directory(directory):\n",
    "    files = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                files.append(f.read())\n",
    "                filenames.append(filename)\n",
    "    return files, filenames\n",
    "\n",
    "def get_top_similar_files(workflow_steps, files, filenames, top_n=3):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([workflow_steps] + files)\n",
    "    similarity_matrix = cosine_similarity(vectorizer[0:1], vectorizer[1:])\n",
    "    similarity_scores = similarity_matrix.flatten()\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "    return [(filenames[i], files[i], similarity_scores[i]) for i in top_indices]\n",
    "\n",
    "def make_inference_request_with_retry(messages, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante l'inferenza: {e}. Tentativo {attempt + 1} di {max_retries}.\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5)  # Aspetta 5 secondi prima di riprovare\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def save_progress(progress, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Progressi salvati in {file_path}\")\n",
    "\n",
    "def load_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "progress = load_progress(progress_file)\n",
    "\n",
    "workflow_files = []\n",
    "workflow_filenames = []\n",
    "\n",
    "for filename in os.listdir(workflow_directory):\n",
    "    file_path = os.path.join(workflow_directory, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                workflow_files.append(json.load(f))  \n",
    "                workflow_filenames.append(filename)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Errore nel caricare il file {filename} come JSON.\")\n",
    "\n",
    "mode = \"half\"  # options: \"half\", \"first_last\", \"last\", \"first_last_compl\"\n",
    "steps_to_predict = []\n",
    "annotations = {}\n",
    "\n",
    "for idx, workflow in enumerate(workflow_files):\n",
    "    print(f\"Elaborazione del workflow file: {workflow_filenames[idx]}\")\n",
    "\n",
    "    if workflow_filenames[idx] in progress:\n",
    "        print(f\"File {workflow_filenames[idx]} giÃ  elaborato, saltando...\")\n",
    "        continue\n",
    "\n",
    "    num_steps = len(workflow[\"steps\"])\n",
    "    if mode == \"half\":\n",
    "        start = num_steps // 2\n",
    "        steps_to_predict = list(range(start, num_steps))\n",
    "    elif mode == \"first_last\":\n",
    "        steps_to_predict = list(range(1, num_steps - 1))\n",
    "    elif mode == \"last\":\n",
    "        steps_to_predict = [num_steps - 1]\n",
    "    elif mode == \"first_last_compl\":\n",
    "        steps_to_predict = list(range(1, num_steps - 1))\n",
    "\n",
    "\n",
    "    for step_id in steps_to_predict:\n",
    "        step = workflow[\"steps\"].get(str(step_id), {})\n",
    "        if \"annotation\" in step:\n",
    "            annotations[step_id] = step[\"annotation\"]\n",
    "\n",
    "    files, filenames = read_files_from_directory(directory)\n",
    "\n",
    "    def validate_prediction(predicted_step, actual_step):\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```', predicted_step, re.DOTALL)\n",
    "        predicted_step_clean = json_match.group(1) if json_match else \"\"\n",
    "\n",
    "        if not predicted_step_clean or not actual_step:\n",
    "            return 0.0  \n",
    "\n",
    "        vectorizer = TfidfVectorizer().fit_transform([predicted_step_clean, actual_step])\n",
    "\n",
    "        if vectorizer.shape[0] < 2:\n",
    "            return 0.0  \n",
    "\n",
    "        similarity = cosine_similarity(vectorizer[0:1], vectorizer[1:])[0, 0]\n",
    "        return similarity\n",
    "\n",
    "    predictions = {}\n",
    "    validation_results = {}\n",
    "\n",
    "    for step_id in steps_to_predict:\n",
    "        step_context = json.dumps(workflow[\"steps\"].get(str(step_id - 1), {})) if step_id > 0 else \"\"\n",
    "        top_similar_files = get_top_similar_files(step_context, files, filenames)\n",
    "        context = \"\\n\\n\".join([f\"File: {filename}\\nContent: {content[:500]}...\" for filename, content, _ in top_similar_files])\n",
    "        file_names = \"\\n\".join([f\"{filename}\" for filename, _, _ in top_similar_files])\n",
    "        annotation_text = annotations.get(step_id, \"\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that generates JSON structures based on given workflow steps.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Given the following workflow step context: {step_context}\\n\\n\" \\\n",
    "                                       f\"The following files are similar:\\n{context}\\n\\n\" \\\n",
    "                                       f\"These are the top 3 similar file names:\\n{file_names}\\n\\n\" \\\n",
    "                                       f\"Annotation: {annotation_text}\\n\\n\" \\\n",
    "                                       f\"Predict the full JSON for step {step_id}. Ensure all fields are included.\"}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            predicted_step = make_inference_request_with_retry(messages)\n",
    "            predictions[step_id] = predicted_step\n",
    "\n",
    "            actual_step = json.dumps(workflow[\"steps\"].get(str(step_id), {}))  \n",
    "            validation_score = validate_prediction(predicted_step, actual_step)\n",
    "            validation_results[step_id] = validation_score\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante l'inferenza per il passo {step_id}: {e}\")\n",
    "            continue \n",
    "\n",
    "    result_data = {}\n",
    "    for step_id in steps_to_predict:\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```', predictions[step_id], re.DOTALL)\n",
    "        json_content = json_match.group(1) if json_match else \"JSON not found\"\n",
    "\n",
    "        result_data[step_id] = {\n",
    "            \"json\": json_content,\n",
    "            \"validation_score\": validation_results[step_id]\n",
    "        }\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    output_file_path = os.path.join(save_directory, f\"predictions_and_validation_results_{workflow_filenames[idx]}.json\")\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    progress[workflow_filenames[idx]] = \"processed\"\n",
    "    save_progress(progress, progress_file)\n",
    "\n",
    "    print(f\"Risultati salvati in: {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd19ea2652b352d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
